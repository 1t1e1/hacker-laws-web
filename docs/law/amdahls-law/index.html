<!doctype html><html class=no-js lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#1b1b1b"><title>Amdahl's Law | My Hugo Site</title><script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script><meta name=description content><link rel=stylesheet href=/hacker-laws-web/css/bundle.css><link rel=icon href=/hacker-laws-web/icons/16.png sizes=16x16 type=image/png><link rel=icon href=/hacker-laws-web/icons/32.png sizes=32x32 type=image/png></head><body><header class=header><a class=logo href=/hacker-laws-web>My Hugo Site</a></header><div class=primary><main class=main><div class="single block"><article class=post><h1 class=post__title>Amdahl's Law</h1><div class=post__content><p><a href=https://en.wikipedia.org/wiki/Amdahl%27s_law>Amdahl&rsquo;s Law on Wikipedia</a></p><blockquote><p>Amdahl&rsquo;s Law is a formula which shows the <em>potential speedup</em> of a computational task which can be achieved by increasing the resources of a system. Normally used in parallel computing, it can predict the actual benefit of increasing the number of processors, which is limited by the parallelisability of the program.</p></blockquote><p>Best illustrated with an example. If a program is made up of two parts, part A, which must be executed by a single processor, and part B, which can be parallelised, then we see that adding multiple processors to the system executing the program can only have a limited benefit. It can potentially greatly improve the speed of part B - but the speed of part A will remain unchanged.</p><p>The diagram below shows some examples of potential improvements in speed:</p><p></p><p><em>(Image Reference: By Daniels220 at English Wikipedia, Creative Commons Attribution-Share Alike 3.0 Unported, <a href=https://en.wikipedia.org/wiki/File:AmdahlsLaw.svg>https://en.wikipedia.org/wiki/File:AmdahlsLaw.svg</a>)</em></p><p>As can be seen, even a program which is 50% parallelisable will benefit very little beyond 10 processing units, whereas a program which is 95% parallelisable can still achieve significant speed improvements with over a thousand processing units.</p><p>As <a href=https://1t1e1.github.io/hacker-laws-web/law/moores-law/>Moore&rsquo;s Law</a> slows, and the acceleration of individual processor speed slows, parallelisation is key to improving performance. Graphics programming is an excellent example - with modern Shader based computing, individual pixels or fragments can be rendered in parallel - this is why modern graphics cards often have many thousands of processing cores (GPUs or Shader Units).</p><p>See also:</p><ul><li><a href=https://1t1e1.github.io/hacker-laws-web/law/brooks-law/>Brooks&rsquo; Law</a></li><li><a href=https://1t1e1.github.io/hacker-laws-web/law/moores-law/>Moore&rsquo;s Law</a></li></ul></div></article></div></main></div><footer class=footer><div class=footer__copyright>Â© 2020 My Hugo Site. <span class=footer__copyright-credits>Powered by <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/vimux/binario rel="nofollow noopener" target=_blank>Binario</a> theme.</span></div></footer></body></html>